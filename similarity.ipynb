{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulri_ndZlNlk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "415470d0-9563-4f13-c8ad-ecf43eec9836"
      },
      "source": [
        "cd drive/My\\ Drive/app/EGR590"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/app/EGR590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeBPy71JinwK"
      },
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZesyEGdulPm"
      },
      "source": [
        "0 - original,\n",
        "1 - random,\n",
        "2 - close,\n",
        "3 - far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15rVU_2bq-85"
      },
      "source": [
        "preproced_docs =[]\n",
        "for file in Path(\"./preproc\").rglob(\"*.txt\"):\n",
        "    with open(file) as f:\n",
        "        txt_file_as_string = f.read()\n",
        "    preproced_docs.append(txt_file_as_string)\n",
        "\n",
        "base_document = preproced_docs[0]\n",
        "documents = preproced_docs[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdyZvr7Uu9Ix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e8c13255-7c46-47dd-c2c3-cf7ef460e4aa"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "reference = base_document\n",
        "comparison_docs = documents\n",
        "\n",
        "def process_tfidf_similarity():\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "\t# To make uniformed vectors, both documents need to be combined first.\n",
        "  embeddings = vectorizer.fit_transform(preproced_docs)\n",
        "  \n",
        "  cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[0:]).flatten()\n",
        "  \n",
        "  highest_score = 0\n",
        "  highest_score_index = 0\n",
        "  \n",
        "  for i, score in enumerate(cosine_similarities):\n",
        "    print(i, score)\n",
        "    if highest_score < score:\n",
        "      highest_score = score\n",
        "      highest_score_index = i\n",
        "  \n",
        "  most_similar_document_rem = documents[highest_score_index]\n",
        "  \n",
        "  print(\"Most similar document by TF-IDF with the score:\", highest_score_index, highest_score)\n",
        "\n",
        "process_tfidf_similarity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.0\n",
            "1 0.8829193112698767\n",
            "2 0.9015492352341855\n",
            "3 0.7004992765627059\n",
            "Most similar document by TF-IDF with the score: 0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaT41V4E1a83"
      },
      "source": [
        "#!wget \"https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed\" universal-sentence-encoder_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4WFgwaG2QKI"
      },
      "source": [
        "#!tar -xvf 4?tf-hub-format=compressed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJM1CbzfzCh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b12c5952-8879-49d3-c6ad-6ddefa5d2594"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def process_use_similarity():\n",
        "  filename = \"./USEmodel\"\n",
        "  model = hub.load(filename)\n",
        "  \n",
        "  base_embeddings = model([base_document])\n",
        "  \n",
        "  embeddings = model(preproced_docs)\n",
        "  \n",
        "  scores = cosine_similarity(base_embeddings, embeddings).flatten()\n",
        "  \n",
        "  highest_score = 0\n",
        "  highest_score_index = 0\n",
        "  for i, score in enumerate(scores):\n",
        "    print(i, score)\n",
        "    if highest_score < score:\n",
        "      highest_score = score\n",
        "      highest_score_index = i\n",
        "      \n",
        "  most_similar_document = documents[highest_score_index]\n",
        "  print(\"Most similar document by USE with the score:\", highest_score_index, highest_score)\n",
        "\n",
        "process_use_similarity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.0\n",
            "1 0.44078398\n",
            "2 0.5280792\n",
            "3 0.43918502\n",
            "Most similar document by USE with the score: 0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jebJqL7m9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d370a50b-d24d-4137-aeda-c7a0de207e67"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def process_bert_similarity():\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  sentences = sent_tokenize(base_document)\n",
        "  base_embeddings_sentences = model.encode(sentences)\n",
        "  base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)\n",
        "  \n",
        "  vectors = []\n",
        "  \n",
        "  for i, document in enumerate(documents):\n",
        "    sentences = sent_tokenize(document)\n",
        "    embeddings_sentences = model.encode(sentences)\n",
        "    embeddings = np.mean(np.array(embeddings_sentences), axis=0)\n",
        "    \n",
        "    vectors.append(embeddings)\n",
        "    print(\"making vector at index:\", i)\n",
        "    \n",
        "  vectors.insert(0, base_embeddings)\n",
        "  scores = cosine_similarity([base_embeddings], vectors).flatten()\n",
        "    \n",
        "  highest_score = 0\n",
        "  highest_score_index = 0\n",
        "  for i, score in enumerate(scores):\n",
        "    print(i, score)\n",
        "    if highest_score < score:\n",
        "      highest_score = score\n",
        "      highest_score_index = i\n",
        "  \n",
        "  most_similar_document = documents[highest_score_index]\n",
        "  print(\"Most similar document by BERT with the score:\", highest_score_index, highest_score)\n",
        "\n",
        "process_bert_similarity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making vector at index: 0\n",
            "making vector at index: 1\n",
            "making vector at index: 2\n",
            "0 1.0000002\n",
            "1 0.827144\n",
            "2 0.90516925\n",
            "3 0.6855231\n",
            "Most similar document by BERT with the score: 0 1.0000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsMSpcAVb1oX"
      },
      "source": [
        "#!wget \"https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/longformer-base-4096.tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imXoIfAJcI7R"
      },
      "source": [
        "#!tar -xvf longformermodel/longformer-base-4096.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qePEDiycSlTZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d3d4730-fca6-4d4b-e1b2-a1d4f1300f3d"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
        "\n",
        "def process_longformer_similarity():\n",
        "\n",
        "  model = LongformerModel.from_pretrained('./longformermodel/longformer-base-4096')\n",
        "  tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "  base_tokenized = torch.tensor(tokenizer.encode(base_document, truncation=True)).unsqueeze(0)  # batch of size 1\n",
        "  base_embedding = model(base_tokenized)\n",
        "  print(base_embedding)\n",
        "  exit(1)\n",
        "  base_embedding = torch.mean(torch.stack(base_embedding, 0))\n",
        "\n",
        "  scores = cosine_similarity(base_embedding, base_embedding).flatten()\n",
        "  print(scores)\n",
        "\n",
        "process_longformer_similarity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./longformermodel/longformer-base-4096 were not used when initializing LongformerModel: ['roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.query_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.weight', 'roberta.encoder.layer.0.attention.self.key_global.bias', 'roberta.encoder.layer.0.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query_global.weight', 'roberta.encoder.layer.1.attention.self.query_global.bias', 'roberta.encoder.layer.1.attention.self.key_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.bias', 'roberta.encoder.layer.1.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.value_global.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.query_global.weight', 'roberta.encoder.layer.2.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.value_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query_global.weight', 'roberta.encoder.layer.3.attention.self.query_global.bias', 'roberta.encoder.layer.3.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.value_global.weight', 'roberta.encoder.layer.3.attention.self.value_global.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query_global.weight', 'roberta.encoder.layer.4.attention.self.query_global.bias', 'roberta.encoder.layer.4.attention.self.key_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.weight', 'roberta.encoder.layer.4.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.key_global.weight', 'roberta.encoder.layer.5.attention.self.key_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.weight', 'roberta.encoder.layer.5.attention.self.value_global.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.bias', 'roberta.encoder.layer.6.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.key_global.bias', 'roberta.encoder.layer.6.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.bias', 'roberta.encoder.layer.7.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.bias', 'roberta.encoder.layer.7.attention.self.value_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query_global.weight', 'roberta.encoder.layer.8.attention.self.query_global.bias', 'roberta.encoder.layer.8.attention.self.key_global.weight', 'roberta.encoder.layer.8.attention.self.key_global.bias', 'roberta.encoder.layer.8.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query_global.weight', 'roberta.encoder.layer.9.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.weight', 'roberta.encoder.layer.9.attention.self.value_global.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query_global.weight', 'roberta.encoder.layer.10.attention.self.query_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.weight', 'roberta.encoder.layer.10.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.bias', 'roberta.encoder.layer.11.attention.self.key_global.weight', 'roberta.encoder.layer.11.attention.self.key_global.bias', 'roberta.encoder.layer.11.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./longformermodel/longformer-base-4096 and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.query_global.weight', 'encoder.layer.0.attention.self.query_global.bias', 'encoder.layer.0.attention.self.key_global.weight', 'encoder.layer.0.attention.self.key_global.bias', 'encoder.layer.0.attention.self.value_global.weight', 'encoder.layer.0.attention.self.value_global.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.query_global.weight', 'encoder.layer.1.attention.self.query_global.bias', 'encoder.layer.1.attention.self.key_global.weight', 'encoder.layer.1.attention.self.key_global.bias', 'encoder.layer.1.attention.self.value_global.weight', 'encoder.layer.1.attention.self.value_global.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query_global.weight', 'encoder.layer.2.attention.self.query_global.bias', 'encoder.layer.2.attention.self.key_global.weight', 'encoder.layer.2.attention.self.key_global.bias', 'encoder.layer.2.attention.self.value_global.weight', 'encoder.layer.2.attention.self.value_global.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.query_global.weight', 'encoder.layer.3.attention.self.query_global.bias', 'encoder.layer.3.attention.self.key_global.weight', 'encoder.layer.3.attention.self.key_global.bias', 'encoder.layer.3.attention.self.value_global.weight', 'encoder.layer.3.attention.self.value_global.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.query_global.weight', 'encoder.layer.4.attention.self.query_global.bias', 'encoder.layer.4.attention.self.key_global.weight', 'encoder.layer.4.attention.self.key_global.bias', 'encoder.layer.4.attention.self.value_global.weight', 'encoder.layer.4.attention.self.value_global.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.query_global.weight', 'encoder.layer.5.attention.self.query_global.bias', 'encoder.layer.5.attention.self.key_global.weight', 'encoder.layer.5.attention.self.key_global.bias', 'encoder.layer.5.attention.self.value_global.weight', 'encoder.layer.5.attention.self.value_global.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query_global.weight', 'encoder.layer.6.attention.self.query_global.bias', 'encoder.layer.6.attention.self.key_global.weight', 'encoder.layer.6.attention.self.key_global.bias', 'encoder.layer.6.attention.self.value_global.weight', 'encoder.layer.6.attention.self.value_global.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.query_global.weight', 'encoder.layer.7.attention.self.query_global.bias', 'encoder.layer.7.attention.self.key_global.weight', 'encoder.layer.7.attention.self.key_global.bias', 'encoder.layer.7.attention.self.value_global.weight', 'encoder.layer.7.attention.self.value_global.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.query_global.weight', 'encoder.layer.8.attention.self.query_global.bias', 'encoder.layer.8.attention.self.key_global.weight', 'encoder.layer.8.attention.self.key_global.bias', 'encoder.layer.8.attention.self.value_global.weight', 'encoder.layer.8.attention.self.value_global.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query_global.weight', 'encoder.layer.9.attention.self.query_global.bias', 'encoder.layer.9.attention.self.key_global.weight', 'encoder.layer.9.attention.self.key_global.bias', 'encoder.layer.9.attention.self.value_global.weight', 'encoder.layer.9.attention.self.value_global.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.query_global.weight', 'encoder.layer.10.attention.self.query_global.bias', 'encoder.layer.10.attention.self.key_global.weight', 'encoder.layer.10.attention.self.key_global.bias', 'encoder.layer.10.attention.self.value_global.weight', 'encoder.layer.10.attention.self.value_global.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.query_global.weight', 'encoder.layer.11.attention.self.query_global.bias', 'encoder.layer.11.attention.self.key_global.weight', 'encoder.layer.11.attention.self.key_global.bias', 'encoder.layer.11.attention.self.value_global.weight', 'encoder.layer.11.attention.self.value_global.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[ 0.1382, -0.6664, -1.4531,  ..., -1.7736, -1.5396,  0.7059],\n",
            "         [ 0.8418,  0.7191, -1.9285,  ..., -2.8762, -0.0929, -0.3925],\n",
            "         [ 2.7008,  0.6449, -1.9766,  ..., -0.3545, -0.7933,  0.4709],\n",
            "         ...,\n",
            "         [ 1.4051,  0.1597, -0.5038,  ..., -1.3898, -1.0105, -0.4175],\n",
            "         [ 2.4175, -0.0375, -0.9381,  ..., -1.0539,  0.1333, -0.2646],\n",
            "         [ 1.6907,  0.4530, -0.2882,  ..., -1.2306, -0.1024, -0.3290]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[-5.0158e-01,  6.9932e-02,  9.2705e-01,  4.6227e-01, -3.7044e-01,\n",
            "         -3.8549e-01,  7.4490e-01, -8.7140e-01, -3.8253e-01,  2.9432e-01,\n",
            "         -3.5054e-01,  1.9231e-01, -1.2834e-01,  1.8306e-01,  2.5327e-01,\n",
            "          3.4146e-01,  4.5417e-01,  6.7930e-01,  4.2687e-01, -8.1116e-01,\n",
            "         -3.6495e-01,  3.6333e-02, -5.7067e-01,  6.0623e-01,  1.4249e-01,\n",
            "          1.8065e-01,  5.0042e-03,  2.0572e-01,  2.7808e-01,  8.8426e-02,\n",
            "          8.7566e-01,  8.5729e-01,  2.8682e-01, -3.3233e-01, -1.6347e-01,\n",
            "          4.6289e-02, -4.0295e-01, -3.9451e-01, -1.9334e-01, -1.0554e-01,\n",
            "         -2.3840e-02, -8.2342e-01, -1.3267e-01,  8.8435e-01, -3.2418e-01,\n",
            "          1.4699e-01, -5.4220e-01,  6.8221e-02,  2.9996e-01,  3.0169e-01,\n",
            "         -5.2461e-01,  6.8818e-01, -6.2451e-01, -1.9459e-01,  2.7281e-01,\n",
            "          4.4233e-01,  7.8770e-01,  8.7628e-01,  1.3396e-01, -5.5683e-01,\n",
            "         -1.2257e-01, -4.0597e-01,  5.7118e-01, -5.9286e-01,  5.2939e-01,\n",
            "         -5.7321e-01,  1.9742e-01, -6.5170e-01, -2.4057e-01,  1.6149e-01,\n",
            "         -5.7975e-01,  6.8290e-01, -1.9139e-01,  7.5575e-01, -6.6049e-01,\n",
            "          1.5951e-01,  4.7025e-01, -6.3248e-01,  1.6029e-01,  1.6901e-01,\n",
            "          1.7861e-01,  1.3924e-01, -1.3072e-01, -3.9768e-01,  8.7013e-01,\n",
            "         -5.1359e-01,  4.5676e-01,  7.1234e-01,  4.9630e-01, -3.3804e-01,\n",
            "         -5.9015e-02,  4.7699e-01, -1.7277e-01, -4.3726e-01,  4.0020e-02,\n",
            "          4.4795e-01, -6.1980e-01, -5.3129e-01, -2.9391e-01, -4.5471e-01,\n",
            "         -2.7347e-01,  7.9532e-01,  5.6084e-01,  8.7490e-01,  8.8008e-01,\n",
            "          3.7848e-01,  1.2773e-01, -3.7958e-02,  1.4803e-01,  3.7844e-01,\n",
            "          4.6728e-01, -9.0961e-02, -4.7450e-02, -1.8190e-01,  8.8053e-01,\n",
            "          3.7885e-01,  1.2916e-01,  1.7538e-02, -7.4543e-01, -4.4973e-01,\n",
            "          6.7340e-01,  1.2431e-01, -3.9761e-01, -4.1072e-01, -3.6184e-01,\n",
            "         -6.6447e-02, -3.8263e-01, -9.4283e-02,  1.6534e-01, -3.2049e-01,\n",
            "          7.2624e-01,  4.1572e-01, -1.3927e-01,  5.2271e-01,  3.7361e-01,\n",
            "          1.8252e-01,  4.4728e-01,  1.0822e-01,  1.3977e-01, -4.5032e-01,\n",
            "          2.0892e-01,  1.6432e-01, -6.5952e-01,  4.8197e-01,  2.7060e-01,\n",
            "         -2.5678e-02,  3.8290e-01,  7.6550e-01, -1.7065e-01, -7.0192e-01,\n",
            "          6.2658e-01, -2.9087e-01,  1.0489e-01, -4.1363e-01,  7.2025e-01,\n",
            "          4.6703e-01, -3.2450e-02,  2.9902e-01,  7.1729e-01,  8.3794e-01,\n",
            "         -4.5703e-01, -3.0470e-01,  1.1398e-01, -2.9382e-01,  2.9191e-01,\n",
            "          1.1587e-02,  7.8207e-01, -2.9790e-01, -6.7289e-01, -9.1407e-02,\n",
            "         -4.4386e-01, -4.9175e-01, -9.6742e-02,  2.4933e-01,  3.3111e-01,\n",
            "         -9.4947e-02, -7.8284e-02, -3.7531e-01,  1.5139e-01, -4.6079e-01,\n",
            "         -5.0825e-01,  5.5047e-01, -1.3876e-01,  2.8126e-01,  7.4929e-01,\n",
            "          1.9232e-01, -2.2717e-01, -8.4011e-01, -1.8718e-01,  2.3976e-02,\n",
            "          2.9020e-01,  3.5508e-01,  3.2780e-01, -7.1109e-01, -3.2071e-01,\n",
            "         -1.1727e-01, -6.4667e-01,  2.1489e-01, -8.8349e-01,  6.7123e-01,\n",
            "         -7.8609e-01,  2.2923e-01,  3.6120e-01,  1.5728e-01,  4.5028e-01,\n",
            "          3.4980e-01, -6.2046e-01,  7.8280e-02,  4.4907e-01, -8.3337e-01,\n",
            "          4.8968e-01, -5.1908e-03, -4.8499e-01, -4.9146e-01, -1.2396e-01,\n",
            "         -2.3634e-01, -2.9695e-02,  4.8931e-01, -3.7829e-01, -3.5255e-01,\n",
            "          1.6241e-01, -2.9185e-01, -2.1111e-01, -3.9102e-02,  4.8489e-01,\n",
            "          1.5503e-01,  1.7838e-01,  2.3214e-01,  6.3187e-01,  2.5488e-01,\n",
            "         -5.7022e-02, -4.1686e-01,  2.7482e-01,  7.1016e-01,  7.7595e-01,\n",
            "          1.2551e-01,  2.2365e-01, -1.0366e-01, -1.6802e-01,  1.6264e-01,\n",
            "         -1.7441e-01, -8.5257e-01,  6.8742e-01, -2.3513e-01, -1.5888e-01,\n",
            "          1.6288e-01,  1.2639e-02, -9.1471e-01,  1.3322e-01, -6.7000e-01,\n",
            "          1.6520e-01, -6.5546e-01, -7.1956e-01,  3.8563e-02, -5.3968e-01,\n",
            "          3.4208e-01, -3.2305e-01,  4.4220e-01, -2.4046e-01, -3.6285e-01,\n",
            "          3.3619e-01,  7.1451e-01, -7.6902e-01,  3.3631e-01, -4.7778e-01,\n",
            "          7.7377e-01, -9.1986e-01,  4.1601e-01, -2.9472e-01, -2.4682e-02,\n",
            "         -7.7614e-02,  5.9198e-01,  2.8140e-01, -5.3459e-01,  4.2896e-01,\n",
            "         -5.8258e-01, -6.5084e-02,  1.8559e-01,  2.9727e-01,  4.3087e-01,\n",
            "         -3.4633e-01,  4.1576e-01, -4.8033e-01, -6.3023e-01, -5.7697e-01,\n",
            "          5.6344e-02,  8.9062e-03, -2.7282e-01,  6.7038e-01, -2.0723e-01,\n",
            "          5.4079e-01,  4.3858e-01, -3.9821e-01,  5.9535e-01, -4.7850e-01,\n",
            "          3.5587e-01,  1.9864e-01,  3.9010e-01,  6.2671e-01,  1.1230e-01,\n",
            "          3.4795e-03,  3.6146e-01, -5.1757e-01,  2.4044e-01, -7.6873e-01,\n",
            "         -5.7677e-01, -1.0824e-01, -3.3557e-01,  6.6044e-01, -1.9417e-01,\n",
            "          2.0778e-01,  1.5587e-01,  2.6759e-01, -1.4085e-01,  4.1217e-01,\n",
            "         -3.7659e-01,  5.1263e-01, -5.0020e-01,  1.4623e-01,  2.0966e-01,\n",
            "          8.0933e-01, -8.1127e-01, -3.0810e-01,  1.0991e-02, -3.9207e-01,\n",
            "         -1.1018e-01,  8.2507e-01, -4.5298e-01,  8.8010e-01,  3.8042e-01,\n",
            "          4.6258e-01,  8.5320e-01, -2.8101e-01, -1.2816e-01,  6.7404e-01,\n",
            "          2.4079e-01, -6.0352e-01,  2.5858e-01, -2.4462e-01, -4.3014e-01,\n",
            "          4.9910e-02, -7.4161e-01,  3.9878e-01,  7.8995e-01, -5.4066e-01,\n",
            "         -2.6505e-02, -6.3831e-01, -3.0958e-01, -5.4779e-01, -7.4932e-01,\n",
            "         -1.7505e-01,  3.0211e-01, -1.5275e-02, -8.0308e-02,  3.7870e-01,\n",
            "         -2.6729e-01, -7.1631e-01, -2.3266e-01,  6.5044e-01, -6.6073e-02,\n",
            "          2.7129e-01,  5.2526e-01,  4.6086e-02,  5.0790e-01,  5.6257e-01,\n",
            "         -4.8445e-01,  5.9203e-01,  2.5061e-01,  2.9015e-01,  5.3176e-01,\n",
            "          7.0043e-01,  2.9765e-01,  5.8171e-01, -3.4398e-01, -1.6951e-01,\n",
            "          1.3744e-01,  4.0816e-01,  5.4619e-01, -3.2331e-01, -3.5607e-01,\n",
            "         -3.2238e-01, -6.5591e-01,  4.3955e-01,  3.7826e-01,  2.7272e-01,\n",
            "         -4.0317e-01,  6.9887e-01,  6.9051e-01, -4.6947e-01, -1.5769e-01,\n",
            "          3.7119e-01,  4.3213e-03,  9.2014e-02, -3.9242e-01,  3.4520e-01,\n",
            "         -5.9188e-01,  4.2135e-01, -2.5808e-01, -1.3574e-01,  5.5387e-01,\n",
            "         -8.6299e-01, -5.9722e-01,  7.4068e-02, -5.3679e-01, -2.3306e-01,\n",
            "         -2.3509e-01,  5.1672e-01,  2.2433e-01, -7.7506e-02,  3.4549e-01,\n",
            "         -1.7394e-01,  6.3331e-02, -2.4017e-01, -3.0287e-01, -3.7629e-02,\n",
            "         -4.1133e-02,  2.4346e-01,  5.7773e-01,  4.8900e-01, -8.6158e-01,\n",
            "         -6.2205e-01,  1.6210e-01,  1.4993e-02,  7.4026e-02, -5.2419e-01,\n",
            "         -1.4956e-01,  1.9913e-01,  2.0769e-01, -7.7329e-01, -2.5616e-01,\n",
            "          3.9461e-01, -4.3294e-02, -4.5777e-01, -4.7085e-01, -1.4595e-01,\n",
            "          4.4595e-01,  6.1809e-01,  6.1903e-01, -5.8150e-01,  5.6068e-02,\n",
            "         -3.8993e-01, -1.3719e-01, -3.0286e-02, -4.7713e-02, -7.3927e-01,\n",
            "          5.1895e-01,  7.3212e-01,  4.8894e-01,  3.7668e-01,  4.0997e-01,\n",
            "         -6.6563e-01,  1.8125e-01, -8.8515e-01, -9.4185e-01, -1.1961e-01,\n",
            "          6.4487e-01, -3.7480e-01, -6.1182e-01, -3.2339e-01,  3.2700e-01,\n",
            "          7.4722e-01,  6.8203e-01, -8.2505e-01,  6.4294e-01, -6.3142e-01,\n",
            "          4.9097e-03,  3.3476e-01,  3.2618e-02,  5.5474e-01, -7.2428e-01,\n",
            "         -2.1501e-01, -4.2197e-02,  2.2757e-01,  6.7154e-01,  1.8186e-01,\n",
            "         -2.5507e-01, -9.6945e-02, -4.2448e-01,  3.9587e-01, -1.5534e-01,\n",
            "         -3.5125e-01,  1.0228e-01,  3.2270e-02,  5.1429e-01, -3.0550e-01,\n",
            "          8.8322e-01,  2.2155e-01,  1.6102e-01,  4.8235e-01,  5.2446e-01,\n",
            "          9.0619e-01,  8.3145e-01, -5.5087e-01, -1.7421e-01,  7.5637e-01,\n",
            "          1.2424e-02,  8.5956e-01,  1.4194e-01,  5.2635e-01, -5.4559e-01,\n",
            "         -4.1622e-02,  1.2211e-01, -1.7699e-01,  4.8434e-01, -9.0454e-01,\n",
            "          2.3284e-01, -5.3731e-01, -7.0826e-01, -6.5265e-01,  4.4170e-01,\n",
            "         -2.9817e-02,  8.6897e-02,  5.4917e-01, -1.4493e-01,  2.3450e-01,\n",
            "         -1.4895e-01,  6.9694e-01, -2.8515e-01,  6.8231e-01,  4.1618e-01,\n",
            "          6.0809e-01,  3.7114e-01,  1.2937e-01, -2.6042e-01,  5.1233e-01,\n",
            "          7.6928e-01,  6.6331e-01, -6.6414e-01,  2.2699e-01, -5.7227e-01,\n",
            "          2.6351e-01,  3.1589e-02,  2.1045e-01, -3.8970e-02,  4.6038e-01,\n",
            "         -1.4738e-01,  7.9486e-01,  3.6457e-01,  3.9787e-01, -7.1015e-01,\n",
            "          1.8075e-01,  1.6611e-01,  5.6603e-01, -2.9449e-01,  6.1664e-02,\n",
            "          3.6176e-01, -2.4234e-01,  8.1236e-01,  6.6845e-01,  5.5419e-01,\n",
            "         -1.1083e-01, -4.1617e-01,  1.7437e-01, -5.5541e-01, -2.0549e-01,\n",
            "          4.0699e-01, -3.0201e-01, -2.4616e-01, -6.0918e-01,  6.4454e-02,\n",
            "         -2.6620e-01,  7.3907e-01,  2.5608e-01,  4.5263e-01,  2.2439e-01,\n",
            "          7.7701e-01,  6.0603e-01, -7.4529e-01,  4.4249e-02, -2.1949e-01,\n",
            "         -7.4835e-01, -3.9380e-01, -2.5131e-01, -3.8316e-01, -2.7120e-01,\n",
            "          4.6350e-01,  6.8814e-01, -5.1208e-02, -3.8965e-01,  3.5767e-02,\n",
            "         -4.1375e-01,  1.7930e-01,  6.9548e-01,  5.8524e-02, -2.6594e-01,\n",
            "          3.5334e-01, -1.4422e-01,  3.0888e-01,  6.9440e-01,  3.9624e-01,\n",
            "         -4.6628e-01, -2.3437e-01, -2.4546e-01,  2.1291e-01,  3.8699e-02,\n",
            "          2.7406e-02,  9.5561e-02,  3.3300e-01, -2.6492e-01, -4.2871e-01,\n",
            "          8.8211e-03,  2.2657e-01, -1.8192e-01, -1.8169e-02,  4.8203e-01,\n",
            "         -8.3432e-01, -4.9039e-01, -4.1424e-01,  3.7613e-02,  2.8316e-03,\n",
            "         -3.6376e-01,  4.1535e-01, -4.6522e-01,  2.7279e-01,  3.8511e-01,\n",
            "          2.1405e-01,  3.2408e-02, -7.3063e-01,  2.5315e-01, -4.7425e-01,\n",
            "          3.4950e-01,  4.1475e-01, -6.5863e-01,  3.8193e-01,  5.0818e-01,\n",
            "         -5.2086e-01, -3.8891e-01, -9.7200e-02,  7.8513e-02, -6.3092e-01,\n",
            "          7.6801e-02,  2.7333e-01, -2.9614e-01, -9.1802e-02, -2.8125e-01,\n",
            "          3.0311e-01,  1.9134e-01, -6.6632e-01, -3.8248e-01, -7.3782e-01,\n",
            "          1.4591e-01,  1.8854e-01,  1.8405e-01,  9.1185e-03, -4.7350e-01,\n",
            "          4.8017e-01, -5.4059e-01,  6.2116e-01, -2.3245e-01, -5.1873e-01,\n",
            "          4.8653e-01, -4.4313e-01, -8.7748e-01,  2.9996e-01, -1.9600e-01,\n",
            "          7.3907e-01,  1.3454e-01,  6.9973e-01,  1.7464e-01,  2.4508e-01,\n",
            "         -5.2281e-01,  6.4496e-01,  2.6319e-01, -6.8577e-01, -5.8075e-01,\n",
            "         -3.8371e-01, -6.8704e-01,  6.1742e-01, -8.7528e-01,  4.0828e-01,\n",
            "          2.2694e-01,  8.8732e-02, -2.8733e-02,  2.7734e-01, -3.6853e-01,\n",
            "         -1.8365e-01,  4.3894e-01, -2.7583e-01,  6.2002e-01, -3.8288e-01,\n",
            "         -8.4479e-02, -2.6375e-01, -7.9762e-02, -8.4138e-01, -4.6303e-02,\n",
            "          7.9583e-02, -5.4415e-01,  7.4360e-01, -1.7967e-01,  4.6669e-01,\n",
            "         -2.7836e-01,  7.6122e-02, -3.2662e-01,  4.2960e-01,  7.2873e-01,\n",
            "          3.5257e-01, -5.1522e-01,  3.3238e-01, -3.7656e-01, -2.8721e-01,\n",
            "         -4.3526e-01, -3.1714e-01, -1.1239e-02, -4.8585e-01,  3.9797e-01,\n",
            "          2.3290e-01,  1.7492e-01, -1.1870e-01,  3.4944e-01, -1.4633e-01,\n",
            "          1.2164e-01, -8.6889e-02, -4.8903e-01,  4.8928e-01,  2.5825e-01,\n",
            "         -5.1592e-01, -8.2661e-01,  2.2290e-01,  6.2282e-01, -6.5977e-01,\n",
            "          2.4340e-01,  1.9094e-03, -5.7054e-01,  6.9629e-01,  6.7934e-01,\n",
            "         -1.3276e-01,  2.7546e-02, -4.4711e-04, -2.9060e-01,  8.8700e-02,\n",
            "         -5.3538e-01,  4.7759e-01,  3.3623e-01, -2.8274e-01, -2.4151e-02,\n",
            "         -4.1046e-01, -7.1539e-01,  5.9266e-01,  3.9677e-01, -6.5275e-01,\n",
            "          2.6165e-02, -2.5833e-01,  1.8912e-01,  3.6902e-01, -3.4222e-01,\n",
            "         -3.9927e-01, -2.5419e-01,  3.0956e-01,  3.4170e-01,  5.8690e-01,\n",
            "         -4.2644e-01,  4.1120e-02,  2.8147e-01,  5.4848e-01,  5.1230e-01,\n",
            "         -5.4284e-01, -4.6303e-01,  2.3110e-01, -8.0479e-01, -1.8744e-01,\n",
            "         -2.6175e-01,  8.0293e-01, -1.1136e-01,  5.7892e-01,  6.6635e-01,\n",
            "         -7.5646e-01, -1.9872e-01,  6.4272e-01]], grad_fn=<TanhBackward>))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a1459ee174df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprocess_longformer_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-a1459ee174df>\u001b[0m in \u001b[0;36mprocess_longformer_similarity\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mbase_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 4096, 768] at entry 0 and [1, 768] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axffob45kOKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}